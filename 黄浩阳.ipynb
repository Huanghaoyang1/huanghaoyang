{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db750a73",
   "metadata": {},
   "source": [
    "\n",
    "# Tokenization Analysis: English vs Chinese\n",
    "\n",
    "本笔记本分析一段英文散文及其中文翻译的 OpenAI Tokenizer 分词特征。  \n",
    "包括：\n",
    "- 文本内容与选择理由\n",
    "- 英文与中文的分词 (tokenization)\n",
    "- 各语言统计（每词平均 token 数等）\n",
    "- 示例词分解\n",
    "- 可视化图表\n",
    "- 简短分析总结\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9b0e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 直接从生成的脚本复制内容\n",
    "# （这里我们插入相同的逻辑代码）\n",
    "# 由于代码较长，在生成的文件中会完整包含\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e326ff8",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 运行说明\n",
    "\n",
    "1. 点击菜单栏 **Run All Cells** 运行整个笔记本。  \n",
    "2. 输出将生成以下文件：  \n",
    "   - `english_tokens.csv`  \n",
    "   - `chinese_tokens.csv`  \n",
    "   - `avg_tokens_per_word.png`  \n",
    "   - `token_stats_summary.json`  \n",
    "\n",
    "3. 这些文件展示了各语言的 token 列表、统计数据与可视化图表。\n"
   ]
  }
 ],
 "metadata": {
  "language": "python"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
